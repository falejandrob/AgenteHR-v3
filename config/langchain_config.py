"""
LangChain configuration simplificado para Azure OpenAI
Optimizado para o3-mini reasoning model
"""
import os
import logging
from typing import Any, Dict, Optional, List
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_core.messages import BaseMessage
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

# Setup logging
logger = logging.getLogger(__name__)


class O3MiniCompatibleChatOpenAI(AzureChatOpenAI):
    """
    Wrapper para AzureChatOpenAI que filtra par√°metros no soportados por o3-mini
    """
    
    def __init__(self, **kwargs):
        # Para o3-mini, filtrar par√°metros no soportados
        deployment = kwargs.get('deployment_name', '').lower()
        if deployment in {"o3-mini", "o3", "o4-mini"}:
            # Remover par√°metros no soportados
            filtered_kwargs = {k: v for k, v in kwargs.items() 
                             if k not in ['temperature', 'top_p', 'frequency_penalty', 'presence_penalty']}
            logger.info(f"üîß Filtrando par√°metros para {deployment}: {list(kwargs.keys())} -> {list(filtered_kwargs.keys())}")
            super().__init__(**filtered_kwargs)
        else:
            super().__init__(**kwargs)
    
    def invoke(self, input_data, config=None, **kwargs):
        """Override invoke para filtrar par√°metros en tiempo de ejecuci√≥n"""
        try:
            # Filtrar kwargs que no son soportados por o3-mini
            deployment = getattr(self, 'deployment_name', '').lower()
            if deployment in {"o3-mini", "o3", "o4-mini"}:
                # Remover par√°metros no soportados del config si existe
                if config and isinstance(config, dict):
                    config = {k: v for k, v in config.items() 
                             if k not in ['temperature', 'top_p', 'frequency_penalty', 'presence_penalty']}
                
                # Remover par√°metros no soportados de kwargs
                filtered_kwargs = {k: v for k, v in kwargs.items() 
                                 if k not in ['temperature', 'top_p', 'frequency_penalty', 'presence_penalty']}
                
                return super().invoke(input_data, config=config, **filtered_kwargs)
            else:
                return super().invoke(input_data, config=config, **kwargs)
                
        except Exception as e:
            logger.error(f"Error en invoke: {e}")
            raise


def validate_config() -> bool:
    """
    Validate Azure OpenAI configuration
    """
    required_vars = [
        'AZURE_OPENAI_ENDPOINT',
        'AZURE_OPENAI_KEY', 
        'AZURE_OPENAI_DEPLOYMENT'
    ]
    
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        logger.error(f"‚ùå Missing environment variables: {missing_vars}")
        return False
    
    logger.info("‚úÖ Azure OpenAI configuration validated")
    return True

def get_azure_llm() -> AzureChatOpenAI:
    """
    Create and configure Azure ChatOpenAI instance optimizado para o3-mini
    """
    try:
        deployment = os.getenv('AZURE_OPENAI_DEPLOYMENT', 'gpt-4o-mini')
        endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
        api_key = os.getenv('AZURE_OPENAI_KEY')
        api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2025-01-01-preview')
        
        logger.info(f"üß† Configurando modelo: {deployment}")
        
        # Configuraci√≥n base
        llm_params = {
            "azure_endpoint": endpoint,
            "api_key": api_key,
            "api_version": api_version,
            "deployment_name": deployment,
            "verbose": False,
        }
        
        # Para o3-mini (reasoning model) usar configuraci√≥n especial
        if deployment and deployment.lower() in {"o3-mini", "o3", "o4-mini"}:
            logger.info("‚ö° Modo reasoning model activado - configuraci√≥n m√≠nima")
            max_completion_tokens = os.getenv('AZURE_OPENAI_MAX_COMPLETION_TOKENS')
            if max_completion_tokens:
                llm_params["model_kwargs"] = {"max_completion_tokens": int(max_completion_tokens)}
                logger.info(f"üéØ Max completion tokens: {max_completion_tokens}")
            
            # Usar la clase compatible con o3-mini
            llm = O3MiniCompatibleChatOpenAI(**llm_params)
            logger.info("üö´ Usando clase compatible con o3-mini (filtros autom√°ticos)")
            
        else:
            # Para modelos est√°ndar
            max_tokens = int(os.getenv('AZURE_OPENAI_MAX_COMPLETION_TOKENS', '1500'))
            llm_params.update({
                "max_tokens": max_tokens,
                "temperature": 0.3,
                "model_kwargs": {}
            })
            logger.info(f"üéØ Max tokens: {max_tokens}")
            llm = AzureChatOpenAI(**llm_params)
        
        logger.info(f"‚úÖ LLM configurado exitosamente: {deployment}")
        return llm
        
    except Exception as e:
        logger.error(f"‚ùå Error configurando LLM: {e}")
        raise

def get_azure_embeddings():
    """
    Configure Azure OpenAI embeddings for vector search
    """
    return AzureOpenAIEmbeddings(
        azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
        api_key=os.getenv('AZURE_OPENAI_KEY'),
        api_version=os.getenv('AZURE_OPENAI_API_VERSION', '2025-01-01-preview'),
        azure_deployment=os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT', 'text-embedding-3-small')
    )
